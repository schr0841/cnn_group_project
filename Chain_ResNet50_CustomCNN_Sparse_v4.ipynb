{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brzbLQsEDNdk"
      },
      "outputs": [],
      "source": [
        "# Import dependencies\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn as skl\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import cv2\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,Dropout, BatchNormalization, Rescaling\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.layers import Input, Average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNmymyLID4zp",
        "outputId": "ffced33f-4550-4bb7-e5ce-a89c7d1cb199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define base directory\n",
        "base_dir = '/content/drive/MyDrive/BOOTCAMP/ColabNotebooks/ProjectWithGreg/Data'\n",
        "\n",
        "# Verify each directory level\n",
        "drive_dir = '/content/drive/MyDrive'\n",
        "bootcamp_dir = os.path.join(drive_dir, 'BOOTCAMP')\n",
        "colab_notebooks_dir = os.path.join(bootcamp_dir, 'ColabNotebooks')\n",
        "project_dir = os.path.join(colab_notebooks_dir, 'ProjectWithGreg')\n",
        "data_dir = os.path.join(project_dir, 'Data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L15oRBDDEHJF",
        "outputId": "7ae66d15-bc92-4692-d7bc-f48b69ad9401"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['valid',\n",
              " 'test',\n",
              " 'train',\n",
              " 'saved_model',\n",
              " 'best_model_resnet.keras',\n",
              " 'best_chained_model_resnet.keras']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Use os.path.join() to concatenate base_dir and 'train', set train_dir to 'Data/train'\n",
        "# os.path.join(base_dir, 'train') takes base_dir, which = 'Data/', and concatenates it with string 'train'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "\n",
        "# Concatenate base_dir and 'test', set test_dir to 'Data/test'\n",
        "# os.path.join(base_dir, 'test') takes base_dir, which = 'Data/', and concatenates it with string 'test'\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# Concatenate base_dir and 'valid', set valid_dir to 'Data/valid'\n",
        "# os.path.join(base_dir, 'valid') takes base_dir, which = 'Data/', and concatenates it with string 'valid'\n",
        "valid_dir = os.path.join(base_dir, 'valid')\n",
        "\n",
        "#Read contents of base_dir directory and return list of names of entries (files and directories) in it\n",
        "os.listdir(base_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFOwjRCKDKs3",
        "outputId": "4c4abe9d-00d0-4332-d176-0e18dd4ca441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 613 files belonging to 4 classes.\n",
            "Found 315 files belonging to 4 classes.\n",
            "Found 72 files belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "# tf.keras.preprocessing.image_dataset_from_directory function generates tf.data.Dataset from image files in directory\n",
        "# convenient way to load image data for training, validation, or testing in format that's easy to work with TensorFlow models\n",
        "\n",
        "#FOR CHAINED MODEL, RESIZE IMAGES to 224, 224, 3 (what ResNet based model expects)\n",
        "\n",
        "training_set = tf.keras.preprocessing.image_dataset_from_directory(     # image_dataset_from_directory method: images automatically labeled based on subdirectory names\n",
        "                                                                        # each subdirectory treated as a class and labels assigned as integers starting from 0\n",
        "\n",
        "train_dir,                  # Purpose: This is directory path where training images are stored\n",
        "                            # Structure: should contain subdirectories, each representing different class; name of each subdirectory will be used as class label for images within it\n",
        "seed=101,\n",
        "image_size=(224, 224),\n",
        "batch_size=32,\n",
        "label_mode='int'           # to work with sparse labels, use 'int' as value for label_mode parameter\n",
        "    )\n",
        "\n",
        "\n",
        "testing_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "test_dir,                   # Purpose: This is directory path where test images are stored\n",
        "                            # Structure: should contain subdirectories, each representing different class; name of each subdirectory will be used as class label for images within it\n",
        "seed=101,\n",
        "image_size=(224, 224),\n",
        "batch_size=32,\n",
        "label_mode='int'           # to work with sparse labels, use 'int' as value for label_mode parameter\n",
        "    )\n",
        "\n",
        "\n",
        "validation_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "valid_dir,                  # Purpose: This is directory path where valid images are stored\n",
        "                            # Structure: should contain subdirectories, each representing different class; name of each subdirectory will be used as class label for images within it\n",
        "seed=101,\n",
        "image_size=(224, 224),\n",
        "batch_size=32,\n",
        "label_mode='int'           # to work with sparse labels, use 'int' as value for label_mode parameter\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVVnM9jUtpcI"
      },
      "outputs": [],
      "source": [
        "#Alterations necessary for chaining both models\n",
        "#(1) Resize images to 224, 224, 3 (what ResNet based model expects)\n",
        "#(2) Remove input_shape from Data Augmentation definition\n",
        "#(3) Remove all MaxPooling2D and Flatten layers from custom model\n",
        "#(4) Remove data augmentation and rescaling from custom_cnn model and move them to complete (chained) model\n",
        "#(5) Add BatchNormalization layer right after base model/at start of custom model to normalize 1D vector output before its fed into dense layers of custom model\n",
        "#(6) Remove extra Dropout layers\n",
        "#(7) Add input layer as first layer of chained model, specify input_shape there\n",
        "#(8) Remove BatchNormalization layer from within custom_cnn\n",
        "#(9) DON'T Insert ConverToTensorLayer between ResNet50 and custom_cnn, otherwise model can't be evaluated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdJgEpdU_tDZ"
      },
      "outputs": [],
      "source": [
        "#Specify img_size, channels, img_shape, and class_count before defining model and data pipeline\n",
        "img_size = (224, 224)       #Resize to 224x224\n",
        "channels = 3\n",
        "img_shape = (img_size[0], img_size[1], channels)\n",
        "class_count = len(training_set.class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHvWtGXB0Pky",
        "outputId": "fc1ec81f-a0c7-45e8-d03e-e21b3dc3ada6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Define ResNet50 base model\n",
        "\n",
        "base_model = ResNet50(\n",
        "    include_top=False,            # remove top classification layer of ResNet50 because data still needs to go through chained model before classifications made\n",
        "    weights=\"imagenet\",           # use weights pre-trained on ImageNet dataset\n",
        "    input_shape=img_shape,        # input_shape=img_shape sets input shape for model\n",
        "    pooling='max')                # Global Max Pooling gives compact feature vector of shape (None, 2048) suitable for dense layers\n",
        "                                  # Without Global Max Pooling: get 4D tensor with spatial dimensions (None, height, width, 2048); requires additional processing to connect to dense layers\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# ResNet50 with include_top=False and pooling='max' produces 2D vector with shape =(None, 2048)\n",
        "# pooling='max' argument ensures output is 2D vector with 2048 features, where individual sample is 1D vector with 2048 features. compatible with dense layers directly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgjCVTtQ03Zo"
      },
      "outputs": [],
      "source": [
        "# Define data augmentation\n",
        "data_augmentation = Sequential([\n",
        "    RandomFlip(\"horizontal\"),\n",
        "    ##input_shape=(224, 224, 3)),    #Omit input_shape here, specify it instead in input layer of chained model\n",
        "    RandomRotation(0.2),\n",
        "    RandomZoom(0.2),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtA7xHDF4g09"
      },
      "source": [
        " Defining custom_CNN model  \n",
        " MaxPooling2D Layers typically used to reduce spatial dimensions (height, width) of feature maps while retaining important information  \n",
        "If using ResNet50 with pooling='max', output is already 1D vector (features extracted from entire image)  \n",
        " additional MaxPooling2D layers unnecessary since feature extraction has already been done  \n",
        " Flatten Layer used to convert 2D feature map into 1D vector, which is required to connect to dense layers  \n",
        "When using pre-trained model with pooling='max', output is already 1D vector, so Flatten layer not needed  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEbUMwSR0lQI"
      },
      "outputs": [],
      "source": [
        "# Define custom_CNN to be integrated with ResNet50\n",
        "\n",
        "custom_cnn = Sequential([\n",
        "    #Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
        "\n",
        "    #MaxPooling2D(pool_size=(2, 2)),          #When using pooling='max', ResNet50 outputs 1D vector with shape (batch_size, 2048), where 2048 is features\n",
        "                                              #extracted by model; need to flatten output if to add dense layers directly (but we're not adding dense layers yet)\n",
        "                                              #If you add additional layers, directly connect to this 1D vector --- NOT TRUE: must convert list to tensors first\n",
        "\n",
        "    #Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
        "    #MaxPooling2D(pool_size=(2, 2)),\n",
        "    #BatchNormalization(),                    #move BatchNormalization layer to before start of custom_cnn or else model can't be evaluated\n",
        "    #Dropout(0.25),\n",
        "    #Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
        "    #MaxPooling2D(pool_size=(2, 2)),\n",
        "    #Dropout(0.25),\n",
        "    #Flatten(),                               #Flatten layer only necessary if base model output is 2D feature map (grid of features); would then convert into 1D vector\n",
        "\n",
        "    Dense(128, activation='relu'),            #Dense layer expects 2D tensor where each row represents single sample with its features\n",
        "                                              #layer has 128 units (neurons), uses ReLU activation function;\n",
        "                                              #operates on 2D tensors where each row (sample) has fixed number of features\n",
        "    Dropout(0.25),                            #dropout layer prevents overfitting by randomly setting fraction (25%) of input units to 0 during training\n",
        "    Dense(class_count, activation='softmax')  # Output layer with number of classes\n",
        "])\n",
        "\n",
        "#class_count variable defines number of output classes for model; sets number of units in final Dense layer of model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5D-WsG-6zpj"
      },
      "source": [
        "tf.keras.Sequential([...]):  \n",
        "Usage: This method is used when you want to refer to Sequential through the TensorFlow library directly.  \n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential([  \n",
        "    tf.keras.layers.Dense(64, activation='relu'),  \n",
        "    tf.keras.layers.Dense(10, activation='softmax')  \n",
        "])  \n",
        "\n",
        "Sequential([...]):  \n",
        "Usage: This method is used when you've specifically imported the Sequential class from tensorflow.keras.models.  \n",
        "\n",
        "from tensorflow.keras.models import Sequential  \n",
        "from tensorflow.keras.layers import Dense  \n",
        "\n",
        "model = Sequential([  \n",
        "    Dense(64, activation='relu'),  \n",
        "    Dense(10, activation='softmax')  \n",
        "])  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without pooling: You get a 4D tensor (None, 7, 7, 2048)  \n",
        "With global pooling: You get a 2D tensor (None, 2048)"
      ],
      "metadata": {
        "id": "g9sK9y2ThDDw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8c4l-0DCUMo"
      },
      "outputs": [],
      "source": [
        "#CHAIN PRE-TRAINED MODEL AND CUSTOM MODEL\n",
        "# Create chained model by chaining ResNet50 with custom_cnn\n",
        "# Notes:\n",
        "  # Add an input shape before the data augmentation layer. For some reason not having this layer caused an error.\n",
        "  # Data augmentation should be applied before rescaling, but both should be only in chained model (remove from custom)\n",
        "  # custom_cnn model expectation: layers added after base model must correctly handle 1D vector\n",
        "  # therefore, adding layers like Dense, Dropout, and BatchNormalization is appropriate\n",
        "  # You do not need Conv2D or Flatten layers after base model; they're for 2D feature maps, not 1D vectors\n",
        "  # Do not need to put BatchNormalization layer in between base model and custom_cnn\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=img_shape),   # Add Input layer as first layer, specify shape here; training threw errors without Input Layer\n",
        "                              # including Input layer can be beneficial for clarity and ensuring compatibility\n",
        "    data_augmentation,        # Apply data augmentation to chained model before rescaling\n",
        "    Rescaling(1./255),        # Apply rescaling to chained model after augmentation\n",
        "    base_model,               # Base model ResNet50, outputs 2D vector with shape =(None, 2048)\n",
        "    custom_cnn                # Custom CNN on top of base model; it's first layer is Dense and can accept base_model output\n",
        "])\n",
        "\n",
        "#Compile model\n",
        "optimizer = Adam()\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#TensorFlow’s SavedModel format doesn’t use specific file extension; creates directory containing multiple files and subdirectories\n",
        "#This format is TensorFlow’s default and is designed for better compatibility and integration with TensorFlow Serving\n",
        "\n",
        "# Define filepath to save best model\n",
        "# to save model to specific directory, provide complete absolute path\n",
        "#Define base directory path if necessary\n",
        "base_dir = '/content/drive/MyDrive/BOOTCAMP/ColabNotebooks/ProjectWithGreg/Data'\n",
        "\n",
        "# Create base directory if it doesn't exist\n",
        "if not os.path.exists(base_dir):\n",
        "    os.makedirs(base_dir)\n",
        "\n",
        "# Define full file path including base directory\n",
        "filepath = os.path.join(base_dir, 'best_chained_model_resnet.keras')\n",
        "\n",
        "model.save(filepath)  # Saves in specified directory\n",
        "\n",
        "# Create ModelCheckpoint callback to save best model based on validation accuracy\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check Output Shape of Each Layer\n",
        "#Use model.summary() function to display output shapes of each layer after model is built; helps identify mismatches in shape\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "nQR82Pw5bHqk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "eb05bf2e-adb5-4c13-f60b-a1d39b63c0fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ sequential (\u001b[38;5;33mSequential\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │      \u001b[38;5;34m23,587,712\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ sequential_1 (\u001b[38;5;33mSequential\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   │         \u001b[38;5;34m262,788\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │      <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ sequential_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,788</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,850,500\u001b[0m (90.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,850,500</span> (90.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m262,788\u001b[0m (1.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">262,788</span> (1.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output shape (None, 2048) from the ResNet50 layer is a 2D tensor.  \n",
        "\n",
        "The first dimension None represents the batch size, which is dynamic and can vary.   \n",
        "The second dimension 2048 is the size of the feature vector output by the ResNet50 model after the global pooling layer (pooling='max' in this case).  \n",
        "This output is a 2D tensor with shape (batch_size, 2048)  "
      ],
      "metadata": {
        "id": "KMX7tkbXiYsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shapes (None, 2048) and (2048,) might seem different, but they are actually compatible due to how dimensions are interpreted in the context of neural network layers.  \n",
        "\n",
        "Tensor Shapes in Neural Networks  \n",
        "(None, 2048): This shape represents a 2D tensor:  \n",
        "None: This dimension is the batch size, which can vary and is handled dynamically during training and inference.  \n",
        "2048: This is the feature dimension or the number of features for each sample in the batch.  \n",
        "(2048,): This shape represents a 1D tensor:  \n",
        "2048: This is the feature dimension or the number of features for each individual sample.  \n",
        "\n",
        "Compatibility  \n",
        "In neural networks, the shape (None, 2048) means that each sample in a batch has 2048 features. When a layer expects an input shape of (2048,), it is designed to handle each individual sample's feature vector independently of the batch size.\n",
        "\n",
        "Here's why they are compatible:  \n",
        "\n",
        "Batch Dimension Handling: The first dimension, None, is flexible and can accommodate any batch size. This flexibility is managed internally by the neural network framework, so the model doesn’t need to know the batch size in advance.  \n",
        "\n",
        "Feature Dimension Matching: The 2048 feature dimension in (None, 2048) matches exactly with the 2048 feature dimension expected by the layer with input_shape=(2048,). The 2048 feature dimension in the layer's input_shape specifies the number of features for each individual sample, not considering the batch size.  \n",
        "\n",
        "How They Work Together  \n",
        "Layer Input Handling: When a neural network layer receives an input tensor with shape (None, 2048), it processes each sample’s feature vector (with shape (2048,)) independently, and it handles the batch dimension automatically.  \n",
        "\n",
        "Sequential Layers: In a Sequential model, you set input_shape=(2048,) for the first layer. This tells the model that each sample in the batch has 2048 features. The batch dimension is implicitly handled, so the model correctly processes tensors of shape (None, 2048).  \n",
        "\n",
        "Batch Dimension: The batch size (None) is managed by the framework.    \n",
        "Feature Dimension: The 2048 feature dimension aligns between the output of ResNet50 and the input expectation of custom_cnn.    \n",
        "Thus, these shapes are compatible as they specify the same feature vector size for each sample, and the batch dimension is handled separately.    "
      ],
      "metadata": {
        "id": "ZU5K8Dw7mPlI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFwAl9XJUFnt",
        "outputId": "0fb5c4a3-ade5-480d-e838-21a4b5299ec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.3415 - loss: 2.4409\n",
            "Epoch 1: val_accuracy improved from -inf to 0.48611, saving model to /content/drive/MyDrive/BOOTCAMP/ColabNotebooks/ProjectWithGreg/Data/best_chained_model_resnet.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 7s/step - accuracy: 0.3434 - loss: 2.4202 - val_accuracy: 0.4861 - val_loss: 1.4885\n",
            "Epoch 2/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.4972 - loss: 1.3830\n",
            "Epoch 2: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 7s/step - accuracy: 0.4970 - loss: 1.3773 - val_accuracy: 0.4722 - val_loss: 1.0158\n",
            "Epoch 3/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.4690 - loss: 1.0330\n",
            "Epoch 3: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 6s/step - accuracy: 0.4697 - loss: 1.0340 - val_accuracy: 0.4167 - val_loss: 1.0164\n",
            "Epoch 4/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5216 - loss: 0.9951\n",
            "Epoch 4: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 7s/step - accuracy: 0.5219 - loss: 0.9968 - val_accuracy: 0.3750 - val_loss: 1.0634\n",
            "Epoch 5/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5191 - loss: 0.9781\n",
            "Epoch 5: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 6s/step - accuracy: 0.5191 - loss: 0.9792 - val_accuracy: 0.4028 - val_loss: 1.0051\n",
            "Epoch 6/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5623 - loss: 0.9631\n",
            "Epoch 6: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 7s/step - accuracy: 0.5619 - loss: 0.9632 - val_accuracy: 0.3611 - val_loss: 1.0201\n",
            "Epoch 7/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5177 - loss: 1.0087\n",
            "Epoch 7: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 7s/step - accuracy: 0.5195 - loss: 1.0083 - val_accuracy: 0.4861 - val_loss: 1.0477\n",
            "Epoch 8/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5625 - loss: 0.9226\n",
            "Epoch 8: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 6s/step - accuracy: 0.5628 - loss: 0.9241 - val_accuracy: 0.4722 - val_loss: 1.0819\n",
            "Epoch 9/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5349 - loss: 0.9808\n",
            "Epoch 9: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 6s/step - accuracy: 0.5354 - loss: 0.9806 - val_accuracy: 0.4722 - val_loss: 1.0674\n",
            "Epoch 10/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5385 - loss: 0.9709\n",
            "Epoch 10: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 7s/step - accuracy: 0.5392 - loss: 0.9714 - val_accuracy: 0.4306 - val_loss: 1.0033\n",
            "Epoch 11/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5340 - loss: 0.9653\n",
            "Epoch 11: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 7s/step - accuracy: 0.5350 - loss: 0.9660 - val_accuracy: 0.3611 - val_loss: 1.0613\n",
            "Epoch 12/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.4716 - loss: 1.0394\n",
            "Epoch 12: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 6s/step - accuracy: 0.4726 - loss: 1.0386 - val_accuracy: 0.4444 - val_loss: 1.0089\n",
            "Epoch 13/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5454 - loss: 0.9682\n",
            "Epoch 13: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 6s/step - accuracy: 0.5461 - loss: 0.9683 - val_accuracy: 0.4861 - val_loss: 1.0044\n",
            "Epoch 14/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5781 - loss: 0.9622\n",
            "Epoch 14: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 6s/step - accuracy: 0.5772 - loss: 0.9628 - val_accuracy: 0.4861 - val_loss: 0.9965\n",
            "Epoch 15/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5613 - loss: 0.9631\n",
            "Epoch 15: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 6s/step - accuracy: 0.5618 - loss: 0.9637 - val_accuracy: 0.4722 - val_loss: 0.9907\n",
            "Epoch 16/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5998 - loss: 0.8830\n",
            "Epoch 16: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 6s/step - accuracy: 0.5988 - loss: 0.8857 - val_accuracy: 0.4722 - val_loss: 0.9799\n",
            "Epoch 17/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5429 - loss: 0.9634\n",
            "Epoch 17: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 6s/step - accuracy: 0.5436 - loss: 0.9638 - val_accuracy: 0.4444 - val_loss: 0.9913\n",
            "Epoch 18/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5446 - loss: 0.9722\n",
            "Epoch 18: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 7s/step - accuracy: 0.5451 - loss: 0.9720 - val_accuracy: 0.4722 - val_loss: 1.0073\n",
            "Epoch 19/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5746 - loss: 0.9680\n",
            "Epoch 19: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 6s/step - accuracy: 0.5749 - loss: 0.9676 - val_accuracy: 0.4722 - val_loss: 0.9807\n",
            "Epoch 20/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5869 - loss: 0.9412\n",
            "Epoch 20: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 7s/step - accuracy: 0.5861 - loss: 0.9422 - val_accuracy: 0.4722 - val_loss: 0.9823\n",
            "Epoch 21/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5176 - loss: 0.9496\n",
            "Epoch 21: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 6s/step - accuracy: 0.5183 - loss: 0.9501 - val_accuracy: 0.4583 - val_loss: 1.0111\n",
            "Epoch 22/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5516 - loss: 0.9401\n",
            "Epoch 22: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 6s/step - accuracy: 0.5514 - loss: 0.9410 - val_accuracy: 0.4306 - val_loss: 0.9899\n",
            "Epoch 23/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.6022 - loss: 0.9263\n",
            "Epoch 23: val_accuracy did not improve from 0.48611\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 6s/step - accuracy: 0.6018 - loss: 0.9268 - val_accuracy: 0.4722 - val_loss: 0.9715\n",
            "Epoch 24/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5615 - loss: 0.9436\n",
            "Epoch 24: val_accuracy improved from 0.48611 to 0.50000, saving model to /content/drive/MyDrive/BOOTCAMP/ColabNotebooks/ProjectWithGreg/Data/best_chained_model_resnet.keras\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 6s/step - accuracy: 0.5620 - loss: 0.9442 - val_accuracy: 0.5000 - val_loss: 1.0264\n",
            "Epoch 25/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5661 - loss: 0.9742\n",
            "Epoch 25: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 7s/step - accuracy: 0.5662 - loss: 0.9735 - val_accuracy: 0.4861 - val_loss: 0.9834\n",
            "Epoch 26/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5607 - loss: 0.9188\n",
            "Epoch 26: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 6s/step - accuracy: 0.5608 - loss: 0.9193 - val_accuracy: 0.4861 - val_loss: 0.9964\n",
            "Epoch 27/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5788 - loss: 0.9374\n",
            "Epoch 27: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 7s/step - accuracy: 0.5789 - loss: 0.9372 - val_accuracy: 0.4722 - val_loss: 0.9764\n",
            "Epoch 28/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5335 - loss: 0.9222\n",
            "Epoch 28: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 7s/step - accuracy: 0.5336 - loss: 0.9238 - val_accuracy: 0.4722 - val_loss: 0.9803\n",
            "Epoch 29/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5639 - loss: 0.9373\n",
            "Epoch 29: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 7s/step - accuracy: 0.5633 - loss: 0.9385 - val_accuracy: 0.4722 - val_loss: 0.9680\n",
            "Epoch 30/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5403 - loss: 0.9327\n",
            "Epoch 30: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 6s/step - accuracy: 0.5417 - loss: 0.9326 - val_accuracy: 0.4722 - val_loss: 0.9766\n",
            "Epoch 31/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5792 - loss: 0.9523\n",
            "Epoch 31: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 7s/step - accuracy: 0.5795 - loss: 0.9506 - val_accuracy: 0.4861 - val_loss: 0.9844\n",
            "Epoch 32/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5821 - loss: 0.8991\n",
            "Epoch 32: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 6s/step - accuracy: 0.5810 - loss: 0.9005 - val_accuracy: 0.4722 - val_loss: 0.9756\n",
            "Epoch 33/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5713 - loss: 0.9339\n",
            "Epoch 33: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 6s/step - accuracy: 0.5714 - loss: 0.9336 - val_accuracy: 0.4722 - val_loss: 0.9732\n",
            "Epoch 34/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5811 - loss: 0.8783\n",
            "Epoch 34: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 7s/step - accuracy: 0.5805 - loss: 0.8802 - val_accuracy: 0.4861 - val_loss: 0.9678\n",
            "Epoch 35/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5815 - loss: 0.9222\n",
            "Epoch 35: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 6s/step - accuracy: 0.5806 - loss: 0.9232 - val_accuracy: 0.4861 - val_loss: 0.9626\n",
            "Epoch 36/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.6146 - loss: 0.8805\n",
            "Epoch 36: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 6s/step - accuracy: 0.6139 - loss: 0.8823 - val_accuracy: 0.4861 - val_loss: 0.9538\n",
            "Epoch 37/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5756 - loss: 0.9186\n",
            "Epoch 37: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 6s/step - accuracy: 0.5744 - loss: 0.9194 - val_accuracy: 0.4722 - val_loss: 0.9809\n",
            "Epoch 38/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.6059 - loss: 0.8986\n",
            "Epoch 38: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 7s/step - accuracy: 0.6052 - loss: 0.8998 - val_accuracy: 0.4861 - val_loss: 0.9577\n",
            "Epoch 39/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.6135 - loss: 0.8778\n",
            "Epoch 39: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 6s/step - accuracy: 0.6126 - loss: 0.8784 - val_accuracy: 0.4861 - val_loss: 0.9758\n",
            "Epoch 40/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5941 - loss: 0.9326\n",
            "Epoch 40: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 7s/step - accuracy: 0.5939 - loss: 0.9319 - val_accuracy: 0.4861 - val_loss: 1.0049\n",
            "Epoch 41/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5837 - loss: 0.9015\n",
            "Epoch 41: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 6s/step - accuracy: 0.5834 - loss: 0.9029 - val_accuracy: 0.4861 - val_loss: 0.9833\n",
            "Epoch 42/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5941 - loss: 0.8823\n",
            "Epoch 42: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 6s/step - accuracy: 0.5934 - loss: 0.8835 - val_accuracy: 0.4583 - val_loss: 0.9925\n",
            "Epoch 43/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5741 - loss: 0.9121\n",
            "Epoch 43: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 7s/step - accuracy: 0.5743 - loss: 0.9124 - val_accuracy: 0.4861 - val_loss: 0.9716\n",
            "Epoch 44/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5628 - loss: 0.9348\n",
            "Epoch 44: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 6s/step - accuracy: 0.5632 - loss: 0.9343 - val_accuracy: 0.4861 - val_loss: 0.9679\n",
            "Epoch 45/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5872 - loss: 0.9125\n",
            "Epoch 45: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 6s/step - accuracy: 0.5870 - loss: 0.9124 - val_accuracy: 0.4861 - val_loss: 0.9706\n",
            "Epoch 46/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5821 - loss: 0.9203\n",
            "Epoch 46: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 6s/step - accuracy: 0.5816 - loss: 0.9207 - val_accuracy: 0.4861 - val_loss: 0.9574\n",
            "Epoch 47/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5814 - loss: 0.8692\n",
            "Epoch 47: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 6s/step - accuracy: 0.5811 - loss: 0.8704 - val_accuracy: 0.4722 - val_loss: 0.9740\n",
            "Epoch 48/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5770 - loss: 0.9046\n",
            "Epoch 48: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 6s/step - accuracy: 0.5773 - loss: 0.9051 - val_accuracy: 0.4861 - val_loss: 0.9694\n",
            "Epoch 49/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.6060 - loss: 0.8912\n",
            "Epoch 49: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 7s/step - accuracy: 0.6055 - loss: 0.8911 - val_accuracy: 0.4583 - val_loss: 1.0040\n",
            "Epoch 50/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5621 - loss: 0.9234\n",
            "Epoch 50: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 6s/step - accuracy: 0.5631 - loss: 0.9223 - val_accuracy: 0.4861 - val_loss: 0.9828\n",
            "Epoch 51/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5773 - loss: 0.8838\n",
            "Epoch 51: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 7s/step - accuracy: 0.5775 - loss: 0.8841 - val_accuracy: 0.4861 - val_loss: 0.9585\n",
            "Epoch 52/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5709 - loss: 0.9552\n",
            "Epoch 52: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 6s/step - accuracy: 0.5716 - loss: 0.9536 - val_accuracy: 0.4861 - val_loss: 0.9928\n",
            "Epoch 53/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5967 - loss: 0.8802\n",
            "Epoch 53: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 7s/step - accuracy: 0.5967 - loss: 0.8806 - val_accuracy: 0.4722 - val_loss: 0.9688\n",
            "Epoch 54/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5550 - loss: 0.9150\n",
            "Epoch 54: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 7s/step - accuracy: 0.5568 - loss: 0.9136 - val_accuracy: 0.4861 - val_loss: 0.9915\n",
            "Epoch 55/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5908 - loss: 0.8701\n",
            "Epoch 55: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 7s/step - accuracy: 0.5910 - loss: 0.8704 - val_accuracy: 0.4861 - val_loss: 0.9764\n",
            "Epoch 56/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5756 - loss: 0.9245\n",
            "Epoch 56: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 6s/step - accuracy: 0.5753 - loss: 0.9236 - val_accuracy: 0.4861 - val_loss: 0.9595\n",
            "Epoch 57/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5986 - loss: 0.8945\n",
            "Epoch 57: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 6s/step - accuracy: 0.5973 - loss: 0.8958 - val_accuracy: 0.4861 - val_loss: 0.9500\n",
            "Epoch 58/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5673 - loss: 0.8926\n",
            "Epoch 58: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 7s/step - accuracy: 0.5674 - loss: 0.8925 - val_accuracy: 0.4861 - val_loss: 0.9844\n",
            "Epoch 59/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5852 - loss: 0.8644\n",
            "Epoch 59: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 6s/step - accuracy: 0.5846 - loss: 0.8654 - val_accuracy: 0.4722 - val_loss: 0.9551\n",
            "Epoch 60/100\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5862 - loss: 0.9032\n",
            "Epoch 60: val_accuracy did not improve from 0.50000\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 7s/step - accuracy: 0.5860 - loss: 0.9028 - val_accuracy: 0.4583 - val_loss: 0.9815\n",
            "Epoch 61/100\n",
            "\u001b[1m 6/20\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 6s/step - accuracy: 0.6067 - loss: 0.8743"
          ]
        }
      ],
      "source": [
        "# Train chained model with added callback\n",
        "history = model.fit(\n",
        "    x=training_set,\n",
        "    epochs=100,\n",
        "    verbose=1,\n",
        "    validation_data=validation_set,\n",
        "    callbacks=[checkpoint]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If KeyboardInterrupt halts training before reaching number of specified epochs, reload saved model, recompile model, and continue training model.\n",
        "\n",
        "If training does not reach 100 epochs but val accuracy did not improve from highest value, move on to evaluating the model."
      ],
      "metadata": {
        "id": "4bc9qry9Loi6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8s2NmGola47O",
        "outputId": "c8c0ef2d-4d79-4ba5-cb7a-aa0a5bcc0fb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "total 189718\n",
            "-rw------- 1 root root 98159789 Aug 13 03:56 best_chained_model_resnet.keras\n",
            "-rw------- 1 root root 96094261 Aug  8 23:27 best_model_resnet.keras\n",
            "drwx------ 2 root root     4096 Jul 31 17:32 saved_model\n",
            "drwx------ 2 root root     4096 Jul 15 23:58 test\n",
            "drwx------ 2 root root     4096 Jul 15 23:58 train\n",
            "drwx------ 2 root root     4096 Jul 15 23:58 valid\n"
          ]
        }
      ],
      "source": [
        "#Prepare to load saved model\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "#Mount Google Drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define base directory\n",
        "base_dir = '/content/drive/MyDrive/BOOTCAMP/ColabNotebooks/ProjectWithGreg/Data'\n",
        "\n",
        "# List contents of directory\n",
        "#!ls -l {directory_path}\n",
        "!ls -l {base_dir}\n",
        "\n",
        "# Verify each directory level\n",
        "drive_dir = '/content/drive/MyDrive'\n",
        "bootcamp_dir = os.path.join(drive_dir, 'BOOTCAMP')\n",
        "colab_notebooks_dir = os.path.join(bootcamp_dir, 'ColabNotebooks')\n",
        "project_dir = os.path.join(colab_notebooks_dir, 'ProjectWithGreg')\n",
        "data_dir = os.path.join(project_dir, 'Data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxIyhI3Nc-uj"
      },
      "outputs": [],
      "source": [
        "# Specify correct path to model\n",
        "model_path = os.path.join(base_dir, 'best_chained_model_resnet.keras')\n",
        "\n",
        "#Load saved model\n",
        "model = load_model(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# *** Skip this cell if not continuing to train model ***\n",
        "\n",
        "#If you tracked number of epochs completed previously (e.g., from logs or previous training history), you can calculate how many more epochs\n",
        "#are needed and specify that number\n",
        "\n",
        "# Suppose you trained for 38 epochs previously and want to train for 12 more\n",
        "previous_epochs = 38\n",
        "additional_epochs = 12\n",
        "\n",
        "# Continue training model\n",
        "history = model.fit(\n",
        "    x=training_set,\n",
        "    epochs=previous_epochs + additional_epochs,\n",
        "    initial_epoch=previous_epochs,  # resume training from where you left off\n",
        "    verbose=1,\n",
        "    validation_data=validation_set,\n",
        "    callbacks=[checkpoint]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "wvQlZgkRBJHf",
        "outputId": "47b19aac-d43f-46f9-d3df-a46981f444c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'training_set' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-31de6604d40c>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Continue training model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m history = model.fit(\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprevious_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0madditional_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprevious_epochs\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# resume training from where you left off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'training_set' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this cell to redefine train_dir, test_dir, valid_dir\n",
        "\n",
        "# Use os.path.join() to concatenate base_dir and 'train', set train_dir to 'Data/train'\n",
        "# os.path.join(base_dir, 'train') takes base_dir, which = 'Data/', and concatenates it with string 'train'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "\n",
        "# Concatenate base_dir and 'test', set test_dir to 'Data/test'\n",
        "# os.path.join(base_dir, 'test') takes base_dir, which = 'Data/', and concatenates it with string 'test'\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# Concatenate base_dir and 'valid', set valid_dir to 'Data/valid'\n",
        "# os.path.join(base_dir, 'valid') takes base_dir, which = 'Data/', and concatenates it with string 'valid'\n",
        "valid_dir = os.path.join(base_dir, 'valid')\n",
        "\n",
        "#Read contents of base_dir directory and return list of names of entries (files and directories) in it\n",
        "os.listdir(base_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV2s6CLTtueB",
        "outputId": "fe62a9cb-bc41-4a0f-f125-bf40f51aaffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['valid',\n",
              " 'test',\n",
              " 'train',\n",
              " 'saved_model',\n",
              " 'best_model_resnet.keras',\n",
              " 'best_chained_model_resnet.keras']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this cell to redefine training_set, testing_set, validation_set\n",
        "\n",
        "# tf.keras.preprocessing.image_dataset_from_directory function generates tf.data.Dataset from image files in directory\n",
        "# convenient way to load image data for training, validation, or testing in format that's easy to work with TensorFlow models\n",
        "\n",
        "#FOR CHAINED MODEL, RESIZE IMAGES to 224, 224, 3 (what ResNet based model expects)\n",
        "\n",
        "training_set = tf.keras.preprocessing.image_dataset_from_directory(     # image_dataset_from_directory method: images automatically labeled based on subdirectory names\n",
        "                                                                        # each subdirectory treated as a class and labels assigned as integers starting from 0\n",
        "\n",
        "train_dir,                  # Purpose: This is directory path where training images are stored\n",
        "                            # Structure: should contain subdirectories, each representing different class; name of each subdirectory will be used as class label for images within it\n",
        "seed=101,\n",
        "image_size=(224, 224),\n",
        "batch_size=32,\n",
        "label_mode='int'           # to work with sparse labels, use 'int' as value for label_mode parameter\n",
        "    )\n",
        "\n",
        "\n",
        "testing_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "test_dir,                   # Purpose: This is directory path where test images are stored\n",
        "                            # Structure: should contain subdirectories, each representing different class; name of each subdirectory will be used as class label for images within it\n",
        "seed=101,\n",
        "image_size=(224, 224),\n",
        "batch_size=32,\n",
        "label_mode='int'           # to work with sparse labels, use 'int' as value for label_mode parameter\n",
        "    )\n",
        "\n",
        "\n",
        "validation_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "valid_dir,                  # Purpose: This is directory path where valid images are stored\n",
        "                            # Structure: should contain subdirectories, each representing different class; name of each subdirectory will be used as class label for images within it\n",
        "seed=101,\n",
        "image_size=(224, 224),\n",
        "batch_size=32,\n",
        "label_mode='int'           # to work with sparse labels, use 'int' as value for label_mode parameter\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kdNAb_gtWsr",
        "outputId": "ec627551-3baa-4e13-a4f8-decd91767a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 613 files belonging to 4 classes.\n",
            "Found 315 files belonging to 4 classes.\n",
            "Found 72 files belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mQ9Y5_weaKV",
        "outputId": "6137a837-7696-4092-85b2-d7050b1bc700"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 7s/step - accuracy: 0.6250 - loss: 0.8627\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4s/step - accuracy: 0.5030 - loss: 0.9682\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 7s/step - accuracy: 0.5497 - loss: 0.9249\n",
            "Train Loss:  0.8534024357795715\n",
            "Train Accuracy:  0.6329526901245117\n",
            "--------------------\n",
            "Validation Loss:  0.9649490714073181\n",
            "Validation Accuracy:  0.5138888955116272\n",
            "--------------------\n",
            "Test Loss:  0.9231241941452026\n",
            "Test Accuracy:  0.5333333611488342\n"
          ]
        }
      ],
      "source": [
        "#Run this cell to evaluate model on all three data sets\n",
        "\n",
        "train_score = model.evaluate(training_set, verbose=1)\n",
        "valid_score = model.evaluate(validation_set, verbose=1)\n",
        "test_score = model.evaluate(testing_set, verbose=1)\n",
        "\n",
        "# Print evaluation results\n",
        "print(\"Train Loss: \", train_score[0])\n",
        "print(\"Train Accuracy: \", train_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Validation Loss: \", valid_score[0])\n",
        "print(\"Validation Accuracy: \", valid_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Test Loss: \", test_score[0])\n",
        "print(\"Test Accuracy: \", test_score[1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# *** Skip this cell if not continuing to train model ***\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Recompile model with same optimizer and loss function\n",
        "optimizer = Adam()  # or whatever optimizer you were using\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Continue training model\n",
        "history = model.fit(\n",
        "    x=training_set,             # your training data\n",
        "    epochs=remaining_epochs,    # the number of epochs you want to continue training for\n",
        "    verbose=1,                  # or adjust as per your preference\n",
        "    validation_data=validation_set,  # your validation data\n",
        "    callbacks=[checkpoint]      # use the same ModelCheckpoint callback to continue saving the best model\n",
        ")"
      ],
      "metadata": {
        "id": "DXwv5Ahk_Gfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You don't necessarily need to use test_steps unless your dataset is a generator or if you are working with very large datasets that don't fit entirely in memory.  \n",
        "\n",
        "train_score = model.evaluate(training_set, steps=test_steps, verbose=1)  \n",
        "valid_score = model.evaluate(validation_set, steps=test_steps, verbose=1)  \n",
        "test_score = model.evaluate(testing_set, steps=test_steps, verbose=1)  \n",
        "\n",
        "---\n",
        "If you're using a generator or a custom data pipeline, you might want to calculate test_steps.\n",
        "\n",
        "In the context of training or evaluating a machine learning model, a step refers to a single batch of data being processed by the model. When you specify a step limit or simply steps, you're defining the number of batches the model should process in a single epoch during training or during evaluation.   \n",
        "\n",
        "Steps per Epoch: During training, steps per epoch is the number of batches the model processes before declaring one epoch complete. If your dataset has N samples and your batch size is B, then the steps per epoch would typically be N/B.  \n",
        "\n",
        "Steps During Evaluation: When evaluating a model, steps determine how many batches of the data will be processed to compute the evaluation metrics like loss and accuracy. For instance, if you have a dataset with 1,000 samples and a batch size of 50, you would typically set the number of steps to 20 (1000/50 = 20).  \n",
        "  \n",
        "Step Limit:A step limit is essentially a maximum number of steps you allow the model to process in a single epoch or during evaluation. If you have a large dataset and you want to limit the number of batches processed per epoch or evaluation run, you can set a step limit.  \n",
        "For example, even if your dataset would require 100 steps to cover all the data, you might set a step limit of 50, meaning only half of the data is processed in each epoch.  \n",
        "\n",
        "When is Step Limit Used?  \n",
        "Large Datasets: When datasets are large and you want to avoid processing the entire dataset in one go, either to save time or to avoid overfitting.  \n",
        "\n",
        "Control Over Training: It allows for more control over training or evaluation processes, especially when the computation time or resources are limited.  \n",
        "\n",
        "Custom Evaluation: In scenarios where you might want to evaluate a model on a subset of the data rather than the entire dataset.    \n",
        "In your code, the concept of a step limit isn’t explicitly mentioned. However, test_steps is calculated based on the total number of samples and the batch size, and this effectively controls how much of the dataset is evaluated in each evaluation run. The test_batch_size calculation is done in such a way that the number of steps remains manageable and doesn't exceed a certain threshold (implicitly ensuring the steps don’t become too large)."
      ],
      "metadata": {
        "id": "7bpXdlBOD6Nu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dkBPftld13i"
      },
      "outputs": [],
      "source": [
        "#ts_length represents total number of samples in testing set\n",
        "#ts_length used to determine batch size and number of steps needed for evaluation\n",
        "#ts_length used to ensure evaluation of model is done efficiently, with optimal batch size and correct number of steps, so entire testing set is processed without exceeding step limit\n",
        "\n",
        "# calculate ts_length by multiplying length of testing_set (len(testing_set)) by batch size and store calculation in variable 'ts_length'\n",
        "##ts_length = len(testing_set) * 32                 # Adjust based on actual batch size\n",
        "\n",
        "# ts_length used to:\n",
        "\n",
        "#1. Calculate Batch Size, (test_batch_size): helps in dynamically calculating appropriate batch size for evaluating model on dataset\n",
        "#test_batch_size: calculates batch size by finding largest factor of ts_length that results in batch size where number of steps <= 80\n",
        "#this ensures batch size chosen in such a way that evaluation process can be done efficiently without exceeding step limit\n",
        "\n",
        "#2. Calculate Evaluation Steps (test_steps): crucial for determining number of steps (batches) required to cover entire dataset during evaluation\n",
        "#test_steps: number of steps calculated by dividing ts_length by test_batch_size\n",
        "#determines how many batches will be processed during evaluation of dataset\n",
        "\n",
        "# Determine batch size and steps for evaluation\n",
        "\n",
        "#calculates optimal test_batch_size for processing a time series data (ts_length)\n",
        "#tailored to process sequences or time series where ts_length represents length of sequence\n",
        "##test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length % n == 0 and ts_length / n <= 80]))\n",
        "\n",
        "##test_steps = ts_length // test_batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "oSvW2DgDCsw7",
        "outputId": "f3bad75b-e212-4df9-8fad-ade153fd32de"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-15-cfd9e07a233d>, line 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-cfd9e07a233d>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    Using generators is beneficial when dealing with large datasets that cannot fit into memory all at once, as generators allow you to load data in batches during training and evaluation. When you have the entire dataset in memory, you can directly evaluate the model on the full dataset without the need for generators.\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#If you did not use a data image generator to create the training, testing, and validation sets of data, and you have the entire datasets loaded into memory,\n",
        "#it is not necessary to use generators for evaluation. Using generators is beneficial when dealing with large datasets that cannot fit into memory all at once,\n",
        "#as generators allow you to load data in batches during training and evaluation. When you have the entire dataset in memory, you can directly evaluate the model\n",
        "#on the full dataset without the need for generators. Therefore, if you have already loaded the training, testing, and validation data into memory and do not need\n",
        "#to process the data in batches, you can use the simpler method for evaluating the model (rather than the one below)\n",
        "\n",
        "#ts_length = len(test_df)\n",
        "#test_batch_size = test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
        "#test_steps = ts_length // test_batch_size\n",
        "#train_score = model.evaluate(train_gen, steps= test_steps, verbose= 1)\n",
        "#valid_score = model.evaluate(valid_gen, steps= test_steps, verbose= 1)\n",
        "#test_score = model.evaluate(test_gen, steps= test_steps, verbose= 1)\n",
        "\n",
        "#print(\"Train Loss: \", train_score[0])\n",
        "#print(\"Train Accuracy: \", train_score[1])\n",
        "#print('-' * 20)\n",
        "#print(\"Validation Loss: \", valid_score[0])\n",
        "#print(\"Validation Accuracy: \", valid_score[1])\n",
        "#print('-' * 20)\n",
        "#print(\"Test Loss: \", test_score[0])\n",
        "#print(\"Test Accuracy: \", test_score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Model on training, testing, and validation sets."
      ],
      "metadata": {
        "id": "GTTAJJGhMEjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without test_steps (Recommended if using tf.data.Dataset):  \n",
        "train_score = model.evaluate(training_set, verbose=1)  \n",
        "valid_score = model.evaluate(validation_set, verbose=1)  \n",
        "test_score = model.evaluate(testing_set, verbose=1)  \n",
        "model.evaluate(): This method will automatically go through the entire dataset, as long as training_set, validation_set, and testing_set are properly batched.\n",
        "\n",
        "training_set, validation_set, testing_set are tf.data.Dataset objects that yield batches of images and labels for evaluation"
      ],
      "metadata": {
        "id": "7X7A3bijJYns"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olzxtKTmaLQ5"
      },
      "source": [
        "\n",
        "Data augmentation and rescaling typically occur before the data is fed into the base_model or any other parts of the neural network.  \n",
        "  \n",
        "Data Augmentation and Rescaling: These preprocessing steps are essential to ensure that the input data is properly formatted and normalized before it reaches the model. Augmentation helps in generalizing the model by artificially increasing the diversity of the training data, and rescaling normalizes the pixel values, which helps in stabilizing and speeding up the training process.\n",
        "  \n",
        "Location in the Pipeline: These steps are applied to the images as they are being loaded and processed, which means they happen before the images are passed to the base_model. The purpose is to ensure that the images are in the right format and scale when they enter the model.  \n",
        "  \n",
        "Illustration of process:  \n",
        "  \n",
        "Image Loading: Images are loaded from the directory.  \n",
        "Data Augmentation and Rescaling: Applied to the images as they are being loaded.  \n",
        "Model Processing: The preprocessed images are then fed into the base_model and subsequently through the custom layers added on top.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6d1TxNCetf3"
      },
      "source": [
        "In a model where you are chaining a custom architecture with a pre-trained model like ResNet50, you need to place the data augmentation and rescaling layers appropriately. Here’s how you should approach it:\n",
        "\n",
        "Steps to Chain a Custom Model with ResNet50\n",
        "Data Augmentation: Data augmentation should be applied to the input images before they are fed into the base model (ResNet50). This preprocessing helps to increase the diversity of the training data by applying transformations like rotations, flips, etc.\n",
        "\n",
        "Rescaling: Rescaling should be applied after data augmentation. This is because the Rescaling layer adjusts the pixel values to the appropriate range expected by the model. For ResNet50, this is usually between 0 and 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0TZ4gSB1RYl"
      },
      "source": [
        "UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
        "  warning indicates that you should use an Input layer as the first layer in your Sequential model, rather than specifying input_shape directly in the first layer. This is the preferred method in TensorFlow/Keras when building Sequential models\n",
        "\n",
        "  You specified input_shape directly in the RandomFlip layer, which is part of the data augmentation process. This can lead to the warning you received. To address this, you should use an Input layer as the first layer in your Sequential model to define the input shape, and remove the input_shape argument from other layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKMDIxEVrcJb"
      },
      "source": [
        "The error you are encountering, AttributeError: 'list' object has no attribute 'shape', is likely due to the way the dataset is being passed to the model for evaluation. The BatchNormalization layer expects a tensor as input, but it seems that a list is being passed instead.\n",
        "\n",
        "In the context of evaluating the model, the issue arises because the BatchNormalization layer expects a tensor as input, but it is receiving a list instead. During training, the model may be able to handle the list output from ResNet without issues because the training process involves backpropagation and gradient descent, which can accommodate certain data formats.\n",
        "\n",
        "However, during evaluation, the model is expected to make predictions on new data without updating its weights. This prediction process requires the data to be in the correct tensor format for the layers to process the information correctly.\n",
        "\n",
        "When the BatchNormalization layer encounters a list instead of a tensor during evaluation, it raises an error because it cannot process the data in that format. This discrepancy in data format between training and evaluation can lead to errors specifically related to the BatchNormalization layer, which requires tensor inputs to perform normalization operations.\n",
        "\n",
        "Therefore, to ensure successful evaluation of the model, it is crucial to pass the evaluation datasets in the correct tensor format, allowing each layer, including BatchNormalization, to process the data appropriately and make predictions accurately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygmS5aFeu7M5"
      },
      "source": [
        "BatchNormalization is a layer in neural networks that normalizes the inputs to a layer for each mini-batch. This helps in stabilizing the learning process and dramatically reduces the number of training epochs required to train deep networks.  \n",
        "  \n",
        "What Batch Normalization Does:  \n",
        "Normalization: It normalizes the output of the previous activation layer by subtracting the batch mean and dividing by the batch standard deviation. This centers the data around zero with a standard deviation of one.\n",
        "\n",
        "Scale and Shift: After normalization, it applies a scaling factor (gamma) and a shift factor (beta), which allows the layer to learn the optimal scale and mean of the inputs. This ensures the layer can represent the identity transformation if necessary.  \n",
        "  \n",
        "Why Use Batch Normalization:  \n",
        "Speed Up Training: By normalizing the inputs, it helps in stabilizing the learning process. This often allows for higher learning rates, leading to faster convergence.  \n",
        "  \n",
        "Regularization: It has a slight regularizing effect, reducing the need for other forms of regularization like dropout. This happens because the mini-batch statistics add some noise to each training step.  \n",
        "  \n",
        "Reduce Internal Covariate Shift: It mitigates the problem of internal covariate shift, where the distribution of each layer's inputs changes during training. By maintaining the inputs to each layer with a more consistent distribution, it simplifies and accelerates training.  \n",
        "  \n",
        "Gradient Propagation: It helps in maintaining gradients in a range that prevents them from vanishing or exploding, making it easier to train deeper networks.  \n",
        "  \n",
        "Example of Batch Normalization in the Model:\n",
        "In your case, BatchNormalization is used after the output of the base model and before the dense layers. This helps in ensuring that the inputs to the dense layers are normalized, improving the training process.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s8FBWxiwDXD"
      },
      "source": [
        "\n",
        "The error message you are encountering indicates that there is an issue with the type of data being passed to the model.evaluate() method. The model.evaluate() method typically expects tensors or numpy arrays as input data, but in this case, it seems like you are passing a dataset object (_PrefetchDataset) instead.\n",
        "\n",
        "To resolve this issue, you need to extract the data from your dataset object before passing it to the model.evaluate() method. You can do this by iterating over the dataset and converting it to tensors or numpy arrays that can be used for evaluation."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}